{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch-model-basics-2 [linear model].ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bIAYbYajk1w9","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","      \n","| Name | Description | Date\n","| :- |-------------: | :-:\n","|Reza Hashemi| Building Model Basics 2nd  | On 23rd of August 2019 | width=\"750\" align=\"center\"></a></p>\n","</div>\n","\n","# Building Blocks of Models\n","- ```nn.Linear```\n","- Nonlinear Activations\n","- Loss functions\n","- Optimizers"]},{"cell_type":"code","metadata":{"id":"GVU5-yp3N89I","colab_type":"code","outputId":"7d8822aa-d733-4b96-f400-b54f1ada0a81","executionInfo":{"status":"ok","timestamp":1566581670117,"user_tz":240,"elapsed":5852,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!pip3 install torch torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yy37hEYOEiQ","colab_type":"code","outputId":"3a3cf404-f031-432a-d003-b5c572b530db","executionInfo":{"status":"ok","timestamp":1566581670362,"user_tz":240,"elapsed":6087,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import pandas as pd\n","import torch, torchvision\n","torch.__version__"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.1.0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"gyv2Sy5WO8lK","colab_type":"code","colab":{}},"source":["import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ewrw93tt2BfV","colab_type":"text"},"source":["## 1. nn.Linear\n","```nn.Linear()``` is one of the basic building blocks of any neural network (NN) model\n","  - Performs linear (or affine) transformation in the form of ```Wx (+ b)```. In NN terminology, generates a fully connected, or dense, layer.\n","  - Two parameters, ```in_features``` and ```out_features``` should be specified\n","  - Documentation: [linear_layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n","  \n","```python\n","torch.nn.Linear(in_features,       # size of each input sample\n","                out_features,     # size of each output sample\n","                bias = True)         # whether bias (b) will be added or not\n","                         \n","```"]},{"cell_type":"code","metadata":{"id":"MZ30Xe3qFEgG","colab_type":"code","outputId":"478742a1-0823-4ec7-bf9b-697f4d15f991","executionInfo":{"status":"ok","timestamp":1566581674219,"user_tz":240,"elapsed":167,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["linear = nn.Linear(5, 1)             # input dim = 5, output dim = 1\n","x = torch.FloatTensor([1, 2, 3, 4, 5])    # 1d tensor\n","print(linear(x))      \n","y = torch.ones(3, 5)                      # 2d tensor\n","print(linear(y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([-1.2518], grad_fn=<AddBackward0>)\n","tensor([[-0.3499],\n","        [-0.3499],\n","        [-0.3499]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TIH5QLu0Pm6h","colab_type":"text"},"source":["## 2. Nonlinear activations\n","PyTorch provides a number of nonlinear activation functions. Most commonly used ones are:\n","```python\n","torch.nn.ReLU()                # relu\n","torch.nn.Sigmoid()         # sigmoid\n","torch.nn.Tanh()        # tangent hyperbolic\n","torch.nn.Softmax()        # softmax\n","```\n","  - Documentation: [nonlinear_activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)"]},{"cell_type":"code","metadata":{"id":"fCyPxp6QFe59","colab_type":"code","outputId":"70d0308c-1262-4a39-df0f-709cd6f51c9e","executionInfo":{"status":"ok","timestamp":1566581675729,"user_tz":240,"elapsed":157,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["relu = torch.nn.ReLU()\n","sigmoid = torch.nn.Sigmoid()\n","tanh = torch.nn.Tanh()\n","softmax = torch.nn.Softmax(dim = 0)   # when using softmax, explicitly designate dimension\n","\n","x = torch.randn(5)     # five random numbers\n","print(x)\n","print(relu(x))       \n","print(sigmoid(x))\n","print(tanh(x))\n","print(softmax(x))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([-0.5488, -0.1497,  0.0791, -1.1919, -0.8415])\n","tensor([0.0000, 0.0000, 0.0791, 0.0000, 0.0000])\n","tensor([0.3661, 0.4626, 0.5198, 0.2329, 0.3012])\n","tensor([-0.4996, -0.1486,  0.0789, -0.8312, -0.6866])\n","tensor([0.1774, 0.2645, 0.3324, 0.0933, 0.1324])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pxfoFJoqUWio","colab_type":"text"},"source":["## 3. Loss Functions\n","There are a number of loss functions that are already implemented in PyTorch. Common ones include:\n","- ```nn.MSELoss```: Mean squared error. Commonly used in regression tasks.\n","- ```nn.CrossEntropyLoss```: Cross entropy loss. Commonly used in classification tasks"]},{"cell_type":"code","metadata":{"id":"Bq8fI8SgVmLX","colab_type":"code","outputId":"f23674e4-2618-4c73-b7ca-d87bb9daf30f","executionInfo":{"status":"ok","timestamp":1566581677046,"user_tz":240,"elapsed":162,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a = torch.FloatTensor([2, 4, 5])\n","b = torch.FloatTensor([1, 3, 2])\n","\n","mse = nn.MSELoss()\n","print(mse(a, b))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(3.6667)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OmEnzUaOVyDD","colab_type":"code","outputId":"80c0fa44-e722-4e2f-f8a7-c217dd06644f","executionInfo":{"status":"ok","timestamp":1566581677811,"user_tz":240,"elapsed":210,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# note that when using CrossEntropyLoss, input has to have (N, C) shape, where\n","# N is the batch size\n","# C is the number of classes\n","a = torch.FloatTensor([[0.5, 0], [4.5, 0], [0, 0.4], [0, 0.1]])   # input\n","b = torch.LongTensor([1, 1, 1, 0])                                # target\n","\n","ce = nn.CrossEntropyLoss()\n","print(ce(a,b))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(1.6856)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rj9Dv9CbXINA","colab_type":"text"},"source":["## 4. Optimizers\n","- ```torch.optim``` provides various optimization algorithms that are commonly used. Some of them are: \n","```python\n","optim.Adagrad   \n","optim.Adam\n","optim.RMSprop\n","optim.SGD\n","```\n","- As arguments, (model) parameters and (optionally) learning rate are passed\n","- Model training process\n","  - ```optimizer.zero_grad()```: sets all gradients to zero (for every training batches)\n","  - ```loss_fn.backward()```: back propagate with respect to the loss function\n","  - ```optimizer.step()```: update model parameters"]},{"cell_type":"code","metadata":{"id":"8HI6YPvDXuli","colab_type":"code","colab":{}},"source":["## how pytorch models are trained with loss function and optimizers\n","\n","# input and output data\n","x = torch.randn(5)\n","y = torch.ones(1)\n","\n","model = nn.Linear(5, 1)  # generate model\n","loss_fn = nn.MSELoss()   # define loss function\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = 0.01)     # create optimizer \n","optimizer.zero_grad()                      # setting gradients to zero\n","loss_fn(model(x), y).backward()            # back propagation\n","optimizer.step()                           # update parameters based on gradients computed"],"execution_count":0,"outputs":[]}]}