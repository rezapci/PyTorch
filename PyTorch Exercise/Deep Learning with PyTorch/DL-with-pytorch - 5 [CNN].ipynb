{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL-with-pytorch - 5 [CNN].ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bIAYbYajk1w9","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","      \n","| Name | Description | Date\n","| :- |-------------: | :-:\n","|Reza Hashemi| Convolutional Neural Networks - 5th  | Finalized on 23rd of August 2019 | width=\"750\" align=\"center\"></a></p>\n","</div>\n","\n","# Convolutional Neural Networks\n","- CIFAR-10 image classification with CNN\n","  - Last time, we have attempted to classify images in the CIFAR-10 dataset with simple CNN having one convolutional & pooling layers. The final result was accuracy score of 0.645.\n","  - Again, let's try to improve the classification performance by implementing deeper CNN with more layers"]},{"cell_type":"code","metadata":{"id":"GVU5-yp3N89I","colab_type":"code","outputId":"b772ea84-e859-4160-9b3d-a81179756f38","executionInfo":{"status":"ok","timestamp":1566587572915,"user_tz":240,"elapsed":2076,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!pip3 install torch torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yy37hEYOEiQ","colab_type":"code","outputId":"86e9333e-47e6-49cb-f4d4-bbe2757570a3","executionInfo":{"status":"ok","timestamp":1566587577846,"user_tz":240,"elapsed":399,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import pandas as pd\n","import torch, torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","torch.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.1.0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"ewrw93tt2BfV","colab_type":"text"},"source":["## 1. Import & process dataset\n","- _CIFAR10_ dataset can be downloaded by ```torchvision```\n","  - [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)"]},{"cell_type":"code","metadata":{"id":"W5anlYa01w3w","colab_type":"code","outputId":"3d44b326-12c5-4665-e708-9daf3eb176d0","executionInfo":{"status":"ok","timestamp":1566587583769,"user_tz":240,"elapsed":5108,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","train_dataset = datasets.CIFAR10(root = \"/\", train = True, download = True, transform = transforms.ToTensor())\n","test_dataset = datasets.CIFAR10(root = \"/\", train = False, download = True, transform = transforms.ToTensor())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["170500096it [00:02, 84307128.85it/s]                               \n"],"name":"stderr"},{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9nznVMdo5edZ","colab_type":"text"},"source":["## 2. Creating CNN model and training\n","\n","- Create and train deeper CNN model, having two convolutional & pooling layers\n","\n","![](https://www.researchgate.net/profile/Qianzhou_Du2/publication/322477802/figure/fig3/AS:582461356511232@1515881017676/Illustration-of-Convolutional-Neural-Network-CNN-Architecture.png)"]},{"cell_type":"code","metadata":{"id":"YEoNQr10kGUW","colab_type":"code","colab":{}},"source":["# create data loaders \n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 128, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 128, shuffle = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQawpMRPI7jm","colab_type":"code","colab":{}},"source":["# create CNN with one convolution/pooling layer\n","class net(nn.Module):\n","  def __init__(self, input_dim, num_filters, conv_kernel_size, pool_kernel_size, stride, padding, num_classes):\n","    super(net, self).__init__()\n","    self.input_dim = input_dim\n","    conv_output_size = int((input_dim - conv_kernel_size + 2 * padding)/stride) + 1   # first conv layer output size\n","    pool_output_size = int((conv_output_size - pool_kernel_size)/stride) + 1          # first pooling layer output size\n","    conv_output_size = int((pool_output_size - conv_kernel_size + 2 * padding)/stride) + 1   # second conv layer output size\n","    pool_output_size = int((conv_output_size - pool_kernel_size)/stride) + 1          # second pooling layer output size\n","    \n","    self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size = conv_kernel_size, stride = stride, padding = padding)     \n","    self.pool1 = nn.MaxPool2d(kernel_size = pool_kernel_size, stride = stride)\n","    self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], kernel_size = conv_kernel_size, stride = stride, padding = padding)     \n","    self.pool2 = nn.MaxPool2d(kernel_size = pool_kernel_size, stride = stride)\n","    self.relu = nn.ReLU()\n","    self.dense = nn.Linear(pool_output_size * pool_output_size * num_filters[-1], num_classes)     \n","    \n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = self.relu(x)\n","    x = self.pool1(x)\n","    x = self.conv2(x)\n","    x = self.relu(x)\n","    x = self.pool2(x)\n","    x = x.view(x.size(0), -1)   # resize to fit into final dense layer\n","    x = self.dense(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rP0Gt5E9ajmd","colab_type":"code","colab":{}},"source":["# hyperparameters\n","DEVICE = torch.device('cuda')\n","INPUT_DIM = 32\n","NUM_FILTERS = (32, 32)\n","CONV_KERNEL_SIZE = 3\n","POOL_KERNEL_SIZE = 3\n","STRIDE = 1\n","PADDING = 1\n","NUM_CLASSES = 10\n","LEARNING_RATE = 1e-3\n","NUM_EPOCHS = 30              "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPBm8qDrSWsi","colab_type":"code","colab":{}},"source":["model = net(INPUT_DIM, NUM_FILTERS, CONV_KERNEL_SIZE, POOL_KERNEL_SIZE, STRIDE, PADDING, NUM_CLASSES).to(DEVICE)\n","criterion = nn.CrossEntropyLoss()   # do not need softmax layer when using CEloss criterion\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEBtAPYCFeic","colab_type":"code","outputId":"98bb0fc6-55fe-49b9-b56b-5707206e059c","executionInfo":{"status":"ok","timestamp":1566587825935,"user_tz":240,"elapsed":226939,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["# training for NUM_EPOCHS\n","for i in range(NUM_EPOCHS):\n","  temp_loss = []\n","  for (x, y) in train_loader:\n","    x, y = x.float().to(DEVICE), y.to(DEVICE)\n","    outputs = model(x)\n","    loss = criterion(outputs, y)\n","    temp_loss.append(loss.item())\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","  print(\"Loss at {}th epoch: {}\".format(i, np.mean(temp_loss)))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Loss at 0th epoch: 1.4810623134798406\n","Loss at 1th epoch: 1.2012358184360787\n","Loss at 2th epoch: 1.1048996875353176\n","Loss at 3th epoch: 1.0310378291112992\n","Loss at 4th epoch: 0.9728409978739746\n","Loss at 5th epoch: 0.9245658644934749\n","Loss at 6th epoch: 0.8730855590242255\n","Loss at 7th epoch: 0.8399296389211475\n","Loss at 8th epoch: 0.8048995495452296\n","Loss at 9th epoch: 0.7791642356101814\n","Loss at 10th epoch: 0.7552188227853507\n","Loss at 11th epoch: 0.7366858548520471\n","Loss at 12th epoch: 0.7143801430911969\n","Loss at 13th epoch: 0.6967860703425639\n","Loss at 14th epoch: 0.6731867749825158\n","Loss at 15th epoch: 0.6541185029174971\n","Loss at 16th epoch: 0.6448789071244048\n","Loss at 17th epoch: 0.6236151499516519\n","Loss at 18th epoch: 0.6079566321714455\n","Loss at 19th epoch: 0.58996188198514\n","Loss at 20th epoch: 0.5754009889977058\n","Loss at 21th epoch: 0.5623295599847193\n","Loss at 22th epoch: 0.5469853096758314\n","Loss at 23th epoch: 0.5329701231263787\n","Loss at 24th epoch: 0.5172345991939535\n","Loss at 25th epoch: 0.5031102461278286\n","Loss at 26th epoch: 0.4842299462279395\n","Loss at 27th epoch: 0.47506315567914176\n","Loss at 28th epoch: 0.46049648820591704\n","Loss at 29th epoch: 0.4543775454201662\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qpAJUiHm529m","colab_type":"text"},"source":["## 3. Evaluation\n","- Evaluate the trained CNN model with accuracy score \n","  - Store probability of each instance to a list and compare it with true y label"]},{"cell_type":"code","metadata":{"id":"txXH3dknFpSx","colab_type":"code","outputId":"7f0cb542-3a02-46ab-ab79-b1b3e18c3b04","executionInfo":{"status":"ok","timestamp":1545894809696,"user_tz":420,"elapsed":3180,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["y_pred, y_true = [], []\n","with torch.no_grad():\n","  for x, y in test_loader:\n","    x, y = x.float().to(DEVICE), y.to(DEVICE)\n","    outputs = F.softmax(model(x)).max(1)[-1]       # predicted label\n","    y_true += list(y.cpu().numpy())                # true label\n","    y_pred += list(outputs.cpu().numpy())   "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HV1s3xf5Frkl","colab_type":"code","outputId":"11e94351-4eed-4c45-8f59-9ad4f62adceb","executionInfo":{"status":"ok","timestamp":1545894811244,"user_tz":420,"elapsed":1147,"user":{"displayName":"Buomsoo Kim","photoUrl":"","userId":"18268696804115368229"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# evaluation result\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_true, y_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6643"]},"metadata":{"tags":[]},"execution_count":28}]}]}