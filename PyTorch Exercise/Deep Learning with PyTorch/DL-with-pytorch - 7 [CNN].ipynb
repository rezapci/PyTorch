{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL-with-pytorch - 7 [CNN].ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bIAYbYajk1w9","colab_type":"text"},"source":["<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","      \n","| Name | Description | Date\n","| :- |-------------: | :-:\n","|Reza Hashemi| Convolutional Neural Networks - 7th  | Finalized on 23rd of August 2019 | width=\"750\" align=\"center\"></a></p>\n","</div>\n","\n","# Convolutional Neural Networks\n","- IMDB review sentiment classification with CNN\n","  - Last time, we have started sentence classification with CNN having only one filter. Here, let's try more complicated CNN architecture with different filter sizes to ameliorate the performance."]},{"cell_type":"code","metadata":{"id":"GVU5-yp3N89I","colab_type":"code","outputId":"45d60682-7912-4194-b57e-ccf37d2c5ddd","executionInfo":{"status":"ok","timestamp":1566587689933,"user_tz":240,"elapsed":3401,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!pip3 install torch torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.2)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yy37hEYOEiQ","colab_type":"code","outputId":"f1e2ee45-aaca-4d31-b719-c2be82be10b5","executionInfo":{"status":"ok","timestamp":1566587690318,"user_tz":240,"elapsed":3767,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#!pip install numpy==1.16.2\n","import numpy as np\n","print(np.__version__)\n","\n","import numpy as np\n","import pandas as pd\n","import torch, torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","torch.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.16.2\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'1.1.0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"ewrw93tt2BfV","colab_type":"text"},"source":["## 1. Import & process dataset\n","- IMDB review dataset for sentiment analysis\n","  - [source](http://ai.stanford.edu/~amaas/data/sentiment/)\n","  - Let's cheat a while and use dataset provided by Keras"]},{"cell_type":"code","metadata":{"id":"SxDHXFEsf5em","colab_type":"code","outputId":"48ebcec2-ca47-4972-80ad-888b00d1b27f","executionInfo":{"status":"ok","timestamp":1566587706525,"user_tz":240,"elapsed":19958,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","\n","num_words = 10000\n","maxlen = 50\n","\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = num_words)\n","\n","X_train = sequence.pad_sequences(X_train, maxlen = maxlen, padding = 'pre')\n","X_test = sequence.pad_sequences(X_test, maxlen = maxlen, padding = 'pre')\n","    \n","print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["(25000, 50) (25000, 50) (25000,) (25000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9nznVMdo5edZ","colab_type":"text"},"source":["## 2. Creating CNN model and training\n","\n","- Create and train CNN model for sentence classification, with three convolutional & max pooling layers concatenated in the end.\n","- Model architecture is adopted from [Kim 2015](https://www.aclweb.org/anthology/D14-1181)\n","\n","![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-a-CNN-Filter-and-Polling-Architecture-for-Natural-Language-Processing.png)"]},{"cell_type":"code","metadata":{"id":"tdmsy2B3weeK","colab_type":"code","colab":{}},"source":["class imdbTrainDataset(torch.utils.data.Dataset):\n","  def __init__(self):\n","    self.X = X_train\n","    self.y = y_train\n","  \n","  def __getitem__(self, idx):\n","    return self.X[idx], self.y[idx]\n","  \n","  def __len__(self):\n","    return len(self.X)\n","  \n","class imdbTestDataset(torch.utils.data.Dataset):\n","  def __init__(self):\n","    self.X = X_test\n","    self.y = y_test\n","  \n","  def __getitem__(self, idx):\n","    return self.X[idx], self.y[idx]\n","  \n","  def __len__(self):\n","    return len(self.X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFBmmDxKw1t2","colab_type":"code","colab":{}},"source":["# create dataset & dataloader instances\n","train_dataset = imdbTrainDataset()\n","test_dataset = imdbTestDataset()\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 128, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 128, shuffle = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQawpMRPI7jm","colab_type":"code","colab":{}},"source":["# create CNN with one convolution/pooling layer\n","class net(nn.Module):\n","  def __init__(self, input_dim, num_words, embedding_dim, num_filters, kernel_size, stride):\n","    super(net, self).__init__()\n","    self.input_dim = input_dim\n","    self.embedding_dim = embedding_dim\n","    \n","    conv_output_size1 = int((input_dim - kernel_size[0])/stride) + 1   # first conv layer output size\n","    conv_output_size2 = int((input_dim - kernel_size[1])/stride) + 1   # first conv layer output size\n","    conv_output_size3 = int((input_dim - kernel_size[2])/stride) + 1   # first conv layer output size\n","        \n","    self.embedding = nn.Embedding(num_words, self.embedding_dim)\n","    \n","    # three convolution & pooling layers\n","    self.conv1 = nn.Conv2d(1, num_filters[0], kernel_size = (kernel_size[0], self.embedding_dim), stride = stride)     \n","    self.pool1 = nn.MaxPool2d((conv_output_size1, 1))                # Max-over-time pooling\n","    self.conv2 = nn.Conv2d(1, num_filters[1], kernel_size = (kernel_size[1], self.embedding_dim), stride = stride)     \n","    self.pool2 = nn.MaxPool2d((conv_output_size2, 1))                # Max-over-time pooling\n","    self.conv3 = nn.Conv2d(1, num_filters[2], kernel_size = (kernel_size[2], self.embedding_dim), stride = stride)     \n","    self.pool3 = nn.MaxPool2d((conv_output_size3, 1))                # Max-over-time pooling\n","    \n","    self.relu = nn.ReLU()\n","    self.dense = nn.Linear(num_filters[0] + num_filters[1] + num_filters[2], 2)     \n","    \n","  def forward(self, x):\n","    x = self.embedding(x)                                   # project to word embedding space\n","    x = x.view(-1, 1, self.input_dim, self.embedding_dim)   # resize to fit into convolutional layer\n","    x1 = self.pool1(self.relu(self.conv1(x)))\n","    x2 = self.pool2(self.relu(self.conv2(x)))\n","    x3 = self.pool3(self.relu(self.conv3(x)))\n","\n","    x = torch.cat((x1, x2, x3), dim = 1)   # concatenate three convolutional outputs\n","    x = x.view(x.size(0), -1)   # resize to fit into final dense layer\n","    x = self.dense(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rP0Gt5E9ajmd","colab_type":"code","colab":{}},"source":["# hyperparameters\n","DEVICE = torch.device('cuda')\n","INPUT_DIM = maxlen\n","NUM_FILTERS = (16, 32, 64) \n","KERNEL_SIZE = (1, 2, 3)\n","STRIDE = 1\n","EMBEDDING_DIM = 50\n","NUM_WORDS = num_words\n","LEARNING_RATE = 1e-3\n","NUM_EPOCHS = 30         "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPBm8qDrSWsi","colab_type":"code","colab":{}},"source":["model = net(INPUT_DIM, NUM_WORDS, EMBEDDING_DIM, NUM_FILTERS, KERNEL_SIZE, STRIDE).to(DEVICE)\n","criterion = nn.CrossEntropyLoss()   # do not need softmax layer when using CEloss criterion\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEBtAPYCFeic","colab_type":"code","outputId":"409a541f-e0c1-4f3f-c773-5146c086c1f8","executionInfo":{"status":"ok","timestamp":1566587787912,"user_tz":240,"elapsed":101293,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["# training for NUM_EPOCHS\n","for i in range(NUM_EPOCHS):\n","  temp_loss = []\n","  for (x, y) in train_loader:\n","    x, y = x.long().to(DEVICE), y.to(DEVICE)  # beware that input to embedding should be type 'long'\n","    outputs = model(x)\n","    loss = criterion(outputs, y)\n","    temp_loss.append(loss.item())\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","  print(\"Loss at {}th epoch: {}\".format(i, np.mean(temp_loss)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loss at 0th epoch: 0.6324634409072448\n","Loss at 1th epoch: 0.5018278180944676\n","Loss at 2th epoch: 0.4194177679565488\n","Loss at 3th epoch: 0.35011860849905985\n","Loss at 4th epoch: 0.29556686879724875\n","Loss at 5th epoch: 0.24471684887397047\n","Loss at 6th epoch: 0.19912726409277137\n","Loss at 7th epoch: 0.16028255231830538\n","Loss at 8th epoch: 0.12408019845583002\n","Loss at 9th epoch: 0.09528727776237897\n","Loss at 10th epoch: 0.07133282300997146\n","Loss at 11th epoch: 0.05265233642896827\n","Loss at 12th epoch: 0.038134651353620756\n","Loss at 13th epoch: 0.028006473964802464\n","Loss at 14th epoch: 0.020876914913747078\n","Loss at 15th epoch: 0.01609226120920966\n","Loss at 16th epoch: 0.0124357185012908\n","Loss at 17th epoch: 0.0097842464693917\n","Loss at 18th epoch: 0.00782318807406617\n","Loss at 19th epoch: 0.006367209493372665\n","Loss at 20th epoch: 0.005227200729696423\n","Loss at 21th epoch: 0.004337878479641311\n","Loss at 22th epoch: 0.003619923552127593\n","Loss at 23th epoch: 0.003041107853760525\n","Loss at 24th epoch: 0.002558622922634288\n","Loss at 25th epoch: 0.002180502034856805\n","Loss at 26th epoch: 0.0018681388951800003\n","Loss at 27th epoch: 0.0016006503975237434\n","Loss at 28th epoch: 0.0013796636188516812\n","Loss at 29th epoch: 0.0011864403263922324\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qpAJUiHm529m","colab_type":"text"},"source":["## 3. Evaluation\n","- Evaluate the trained CNN model with accuracy score \n","  - Store probability of each instance to a list and compare it with true y label"]},{"cell_type":"code","metadata":{"id":"txXH3dknFpSx","colab_type":"code","outputId":"a673bf74-ba67-4170-83a1-a28ae2909deb","executionInfo":{"status":"ok","timestamp":1566587787913,"user_tz":240,"elapsed":101285,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["y_pred, y_true = [], []\n","with torch.no_grad():\n","  for x, y in test_loader:\n","    x, y = x.long().to(DEVICE), y.to(DEVICE)       # beware that input to embedding should be type 'long'\n","    outputs = F.softmax(model(x)).max(1)[-1]       # predicted label\n","    y_true += list(y.cpu().numpy())                # true label\n","    y_pred += list(outputs.cpu().numpy())   "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HV1s3xf5Frkl","colab_type":"code","outputId":"24fb1cda-cb8f-43fc-ec5b-f4541f4ae0b9","executionInfo":{"status":"ok","timestamp":1566587787913,"user_tz":240,"elapsed":101279,"user":{"displayName":"Reza Hashemi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrdPXZmzhldkZcu5l9nrnO7t-Ls96No7O8kRuZ=s64","userId":"14585138350013583795"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# evaluation result\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_true, y_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7956"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"8lTotuExys_j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}